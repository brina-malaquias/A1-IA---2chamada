# A1-IA---2chamada
# MLP â€“ Problema de Paridade de 3 Bits

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o e anÃ¡lise de uma Rede Neural Multicamadas (MLP) aplicada ao problema lÃ³gico da **paridade de 3 bits**. O objetivo Ã© investigar como diferentes combinaÃ§Ãµes de hiperparÃ¢metros afetam:

- A velocidade de convergÃªncia (nÃºmero de Ã©pocas atÃ© estabilizaÃ§Ã£o do erro).
- O valor do erro mÃ©dio quadrÃ¡tico final (MSE).
- PossÃ­veis instabilidades ou estagnaÃ§Ãµes no aprendizado.

---

## ğŸ”§ Metodologia

- **Arquitetura**  
  - Camada de entrada: 3 neurÃ´nios (cada bit).  
  - Camada oculta: variÃ¡vel (2, 3, 4 ou 6 neurÃ´nios).  
  - Camada de saÃ­da: 1 neurÃ´nio (saÃ­da binÃ¡ria).  
- **FunÃ§Ã£o de ativaÃ§Ã£o**: Sigmoid  
- **FunÃ§Ã£o de perda**: Mean Squared Error (MSE)  
- **OtimizaÃ§Ã£o**: Gradient Descent (learning rate customizÃ¡vel)  
- **Dados de treinamento**: Todas as 8 combinaÃ§Ãµes possÃ­veis de 3 bits  
  - SaÃ­da = 1 seÂ o nÃºmero de bits â€œ1â€ for Ã­mpar  
  - SaÃ­da = 0 se for par  

---

## âš™ï¸ HiperparÃ¢metros Testados

| Exp | NeurÃ´nios (oculta) | Learning Rate | Ã‰pocas | ObservaÃ§Ãµes gerais                  |
|-----|--------------------|---------------|--------|-------------------------------------|
| 1   | 2                  | 0.1           | 3â€¯000  | ConvergÃªncia lenta; MSE final alto  |
| 2   | 4                  | 0.3           | 5â€¯000  | Boa convergÃªncia; MSE final baixo   |
| 3   | 6                  | 0.3           | 5â€¯000  | Melhor estabilidade e menor MSE     |
| 4   | 4                  | 0.5           | 5â€¯000  | OscilaÃ§Ãµes iniciais; converge razoÃ¡vel |
| 5   | 4                  | 0.1           | 10â€¯000 | Queda tardia do erro; similar ao Expâ€¯2 |
| 6   | 3                  | 0.2           | 3â€¯000  | ConvergÃªncia tardia; MSE intermediÃ¡rio |

---

## ğŸ“ˆ Resultados de ConvergÃªncia

Cada grÃ¡fico abaixo mostra a evoluÃ§Ã£o do **MSE por Ã©poca** para o experimento correspondente. As figuras estÃ£o em `./images/`.

### Experimento 1  
> `H=2, LR=0.1, Ã‰p=3â€¯000`
> 
(![Teste 1](https://github.com/user-attachments/assets/3c21082c-068e-4e66-9cc9-3a5f0b745840))

(![Teste 1](https://github.com/user-attachments/assets/aeac40b8-6cec-4537-b47a-aa5c2e4dce06))

### Experimento 2  
> `H=4, LR=0.3, Ã‰p=5â€¯000`
> 
(![Teste 2](https://github.com/user-attachments/assets/d8d60a8b-8a85-47f1-80b3-0e3f3cf57ccd))

(![Teste 2](https://github.com/user-attachments/assets/d775e76a-57a6-4e6f-9ccd-dd1deeffa2cf))

### Experimento 3  
> `H=6, LR=0.3, Ã‰p=5â€¯000`
> 
(![Teste 3](https://github.com/user-attachments/assets/fb16dd54-cdf1-4439-a884-2ca8a44100fe))

(![Teste 3](https://github.com/user-attachments/assets/145ba71e-5586-426a-921b-8db846f56490))


### Experimento 4  
> `H=4, LR=0.5, Ã‰p=5â€¯000`
> 
(![Teste 4](https://github.com/user-attachments/assets/201d099e-942c-4408-997c-8c4432e67832))

(![Teste 4](https://github.com/user-attachments/assets/130fe391-d281-4a0d-9820-222a1c182b45))

### Experimento 5  
> `H=4, LR=0.1, Ã‰p=10â€¯000`
> 
(![Teste 5](https://github.com/user-attachments/assets/12ea6a13-5d0a-46d9-8b07-5ffa6c5411ce))

(![Teste 5](https://github.com/user-attachments/assets/5ef9cb8e-7a3e-42d6-99ef-b54386f07524))


### Experimento 6  
> `H=3, LR=0.2, Ã‰p=3â€¯000`
>  
(![Teste 6](https://github.com/user-attachments/assets/b8709f3e-2fe8-4c54-bd10-6b7db388c07f))

(![Teste 6](https://github.com/user-attachments/assets/4361efd9-b5ac-4b60-8a13-aba24003368f))

---

## ğŸ” AnÃ¡lise dos Resultados

- **Experimento 3** (`6 neurons`, `LR=0.3`): melhor **MSE final** e curva mais suave, mostrando capacidade de modelar a nÃ£oâ€‘linearidade da paridade.  
- **Experimento 2** (`4 neurons`, `LR=0.3`): tambÃ©m bom desempenho, mas ligeiramente acima do Expâ€¯3 em MSE.  
- **Experimento 4** (`LR=0.5`): converge rÃ¡pido apÃ³s oscilaÃ§Ãµes iniciais, mas nÃ£o supera LR=0.3.  
- **Experimentos com LR baixo (0.1)**: exigem muito mais Ã©pocas para alcanÃ§ar MSE equivalente (Expsâ€¯1 eâ€¯5).  
- **Experimento 6** (`3 neurons`): capacidade intermediÃ¡ria, porÃ©m nÃ£o alcanÃ§a a precisÃ£o das redes com 4â€“6 neurÃ´nios.

---

## ğŸ ConclusÃ£o

- A configuraÃ§Ã£o **[6 neurÃ´nios na oculta + learning rate 0.3 + 5â€¯000 Ã©pocas]** apresentou o melhor equilÃ­brio entre **rapidez de convergÃªncia** e **baixo MSE**.
- Para aplicaÃ§Ãµes que exigem menor custo computacional, **4 neurÃ´nios + LR 0.3** tambÃ©m Ã© uma boa escolha.

---
